{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yasa\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = h5py.File(\"X_train.h5\", \"r\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "'''\n",
    "def lissage(Ly,p):\n",
    "    #Fonction qui débruite une courbe par une moyenne glissante\n",
    "    #sur 2P+1 points\n",
    "    Lyout=[]\n",
    "    for index in range(p, len(Ly)-p):\n",
    "        if index == p:\n",
    "            average = np.mean(Ly[index-p : index+p+1])\n",
    "        else:\n",
    "            average -= Ly[index - 1] / (2 * p + 1)\n",
    "            average += Ly[index + p] / (2 * p + 1)\n",
    "        Lyout.append(average)\n",
    "    return Lyout\n",
    "\n",
    "def nbmax(a):\n",
    "    N=len(a)\n",
    "    M=len(a[0])\n",
    "    res = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        y = lissage(a[i], 20)\n",
    "        nb=0\n",
    "        e=y[0]\n",
    "        f=y[1]\n",
    "        g=y[2]\n",
    "        for j in range(3,len(y)):\n",
    "            e=f\n",
    "            f=g\n",
    "            g=y[j]\n",
    "            if (f>e and f>g):\n",
    "                nb+=1\n",
    "        res[i] = nb\n",
    "    return res\n",
    "'''\n",
    "def deltabeta(data):\n",
    "    N = len(data)\n",
    "    sf = 125\n",
    "    res = np.zeros((N, 2))\n",
    "    for i in range(N):\n",
    "        band = yasa.bandpower(data[i, 11:], sf, kwargs_welch={})\n",
    "        d = band[\"Delta\"][\"CHAN001\"]\n",
    "        b = band[\"Beta\"][\"CHAN001\"]\n",
    "        res[i][0] = d\n",
    "        res[i][1] = d / b\n",
    "    return res\n",
    "\n",
    "def extract_features(h5):\n",
    "    data = h5[\"features\"][:]\n",
    "    features = []\n",
    "    features.append(data[:, :])\n",
    "    #features.append(data[:, :11])\n",
    "    features.append(data[:, 11:].max(1).reshape(-1, 1))\n",
    "    features.append(data[:, 11:].min(1).reshape(-1, 1))\n",
    "    features.append(np.abs(data[:, 11:]).mean(1).reshape(-1, 1))\n",
    "    features.append(deltabeta(data))\n",
    "    ##features.append(nbmax(data[:, 11:]).reshape(-1, 1))\n",
    "    features = np.concatenate(features, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = X_train[\"features\"][:]\n",
    "#deltabeta(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "features_train = extract_features(X_train)\n",
    "# Données\n",
    "#X_train,X_test,y_train,y_test=train_test_split(features_train,y_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=500, n_jobs=-1, num_leaves=10, objective=None,\n",
       "               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0,\n",
       "               verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import lightgbm as lgb\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#clf = RandomForestClassifier(n_estimators=100,n_jobs=2,max_features=\"sqrt\",max_depth=4,min_samples_split=None,verbose=1)\n",
    "#clf = RandomForestClassifier(n_estimators=10,verbose=1,n_jobs=-1)\n",
    "clf = lgb.LGBMClassifier(n_estimators=500,num_leaves=10,learning_rate=0.1,random_state=42,verbose=1)\n",
    "'''clf= MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam',\\\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant',\\\n",
    "                   learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True,\\\n",
    "                   random_state=None, tol=0.0001, verbose=False, warm_start=False,\\\n",
    "                   momentum=0.9, nesterovs_momentum=True, early_stopping=False,\\\n",
    "                   validation_fraction=0.1, beta_1=0.9, \\\n",
    "                   beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
    "'''\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#clf = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski',metric_params=None, n_jobs=None)\n",
    "#from sklearn.svm import SVC\n",
    "#clf = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "#scores=cross_val_score(clf, features_train, y_train, cv=3)\n",
    "#clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#clf = GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "clf.fit(features_train, y_train[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_pred=clf.predict(X_test)\\nimport numpy as np\\nprint((np.array(y_test)))\\nmatches=[int(y_pred[k]==np.array(y_test)[k]) for k in range(len(y_pred))]\\nprint(np.sum(matches)/len(y_pred))'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''y_pred=clf.predict(X_test)\n",
    "import numpy as np\n",
    "print((np.array(y_test)))\n",
    "matches=[int(y_pred[k]==np.array(y_test)[k]) for k in range(len(y_pred))]\n",
    "print(np.sum(matches)/len(y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "#X_test = h5py.File(\"X_test.h5\", \"r\")\n",
    "#features_test = extract_features(X_test)\n",
    "y_pred = clf.predict(features_test)\n",
    "with open(\"y_benchmark_0944.csv\", \"w\") as f:\n",
    "    f.write(\"\".join([\"id,label\\n\"] + [\"{},{}\\n\".format(i, y) for i, y in enumerate(y_pred)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
